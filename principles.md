# Accessibility and responsible AI

## Accessibility guidelines

Accessibility is key for helping as many people around the world as possible. Here are some useful, but non-exhaustive, starting points to be mindful of. Please consider how you can ensure maximum support for different users who might have different devices, or english as a second language, or is colour blind -- as just a few of many examples.

- Can you support keyboard interaction -- interactive elements being accessible via keyboard, logical tab orders, visible focus indicators, and keyboard shortcuts
- Bear in mind visual accessibility -- sufficient colour contrast between text and backgrounds, as well as not relying on colour only for information (this can be very difficult for colour blind people). Provide alternatives to text, and ensure text is resizable or sufficiently large without breaking the layout or functionality.
- Provide clear and descriptive headings and tooltips
- Ensure error messages and instructions are clear, simple, and non-technical
- Think about the devices you want your users to access this on -- is it a big screen, or a small mobile device?
- Seek feedback -- test your tool out early and often to find out anything that is difficult or confusing

## Responsible AI checklist

Here's a checklist for responsible AI systems to bear in mind as you develop your solution.

**Data Privacy and Security:**  
  - Ensure that the data used for training and retrieval is obtained legally and ethically (CPR's is -- you can read more about our methodology [here](https://github.com/climatepolicyradar/methodology)).  
  - Assess how you want to deal with toxic, malicious, or otherwise harmful user input  
  - Anonymize or remove personally identifiable information (PII) from the data

**Ethics, Bias and Fairness:**  
  - Be aware of potential biases in the data and actively work to mitigate them. How might biases creep in?   
  - Try to ensure your tool does not perpetuate or amplify harmful stereotypes or generate discriminatory outputs  
  - Consider the potential societal impact and unintended consequences of the tools outputs. How can you mitigate these? How could your tool be misused?  
  - Try to ensure that the generated content does not cause harm or mislead users.

**Transparency and Explanability:**  
  - Trust is one of the main predictors of user satisfaction with generative AI tools. More than that, this is a high-stakes social impact domain. You must be transparent about the source of the retrieved information, and provide ways to help users work out how trustworthy the output information is. Any ways you can support the user, explain where things came from, or provide context will help.  
  - Allow users to provide feedback and report any concerning or inappropriate outputs.

**Intellectual Property Rights:**  
  - Respect the intellectual property rights of others when using external data sources for retrieval.  
  - Ensure that the tool does not infringe upon copyrights, trademarks, or other legal protections.  
  - Properly attribute and give credit to the original sources of retrieved information.

**User Education and Informed Consent:**  
  - Provide clear information to users about the nature and capabilities of the tool.  
  - Educate users about the potential limitations and risks associated with the generated content

Remember, this checklist is just a starting point, and you may need to adapt and expand it based on the specific requirements and context of your hackathon project.
